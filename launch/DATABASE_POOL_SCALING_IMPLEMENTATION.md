# üîß DATABASE CONNECTION POOL SCALING IMPLEMENTATION

**Implementation Date**: February 1, 2026  
**System Version**: 1.0.0 ‚Üí 1.0.1 (configuration change only)  
**Authority**: Production Engineering Lead  
**Change Type**: REQUIRED - Database connection pool scaling

---

## üéØ OBJECTIVE

Scale database connection pool from 50 to 200 connections to support 50,000 customers with acceptable performance under sustained load.

**Evidence**: Production Audit identified 90% pool utilization at peak load, requiring scaling to prevent performance degradation.

---

## üìã PRE-IMPLEMENTATION CHECKLIST

**Before making ANY changes, verify**:

- [ ] Current system status: STABLE (no active incidents)
- [ ] Current error rate: <0.1%
- [ ] Current p95 latency: <500ms
- [ ] Database health: HEALTHY (no replication lag, no locks)
- [ ] Backup verified: Last backup <24 hours, restoration tested
- [ ] Rollback plan documented and tested
- [ ] Monitoring dashboards active and visible
- [ ] On-call engineer available (30-minute response time)
- [ ] Change window: Low-traffic period (2-6 AM UTC)

**DO NOT PROCEED** if any checklist item fails.

---

## üîç CURRENT CONFIGURATION SNAPSHOT

### Before Change (Baseline)

**Database Connection Pool Settings** (Capture from actual config):

```typescript
// server/config/database.ts or equivalent
const poolConfig = {
  min: 10,              // Minimum connections
  max: 50,              // Maximum connections (CURRENT)
  idleTimeoutMillis: 30000,
  connectionTimeoutMillis: 2000,
  acquireTimeoutMillis: 10000
};
```

**Current Metrics** (Capture from monitoring):
- Active connections: [RECORD ACTUAL VALUE]
- Pool utilization: [RECORD ACTUAL %]
- Connection wait time p95: [RECORD ACTUAL VALUE]
- Error rate: [RECORD ACTUAL %]
- p95 latency: [RECORD ACTUAL VALUE]

**Timestamp**: [RECORD TIMESTAMP]  
**Operator**: [RECORD NAME]

---

## üöÄ IMPLEMENTATION STEPS

### Step 1: Capture Pre-Change State

**Action**: Record all baseline metrics

```bash
# Capture current database connections
psql -U accubooks -d accubooks_production -c "SELECT count(*) FROM pg_stat_activity WHERE datname='accubooks_production';"

# Capture current pool stats from application logs
grep "pool" /var/log/accubooks/app.log | tail -100

# Capture current performance metrics
curl http://localhost:3000/api/health/metrics > pre_change_metrics.json
```

**Evidence Location**: `launch/evidence/pre_change_snapshot_[TIMESTAMP].json`

---

### Step 2: Update Configuration

**File to Modify**: `server/config/database.ts` (or equivalent)

**Change**:

```typescript
// BEFORE (v1.0.0)
const poolConfig = {
  min: 10,
  max: 50,              // OLD VALUE
  idleTimeoutMillis: 30000,
  connectionTimeoutMillis: 2000,
  acquireTimeoutMillis: 10000
};

// AFTER (v1.0.1)
const poolConfig = {
  min: 20,              // Increased minimum (20% of max)
  max: 200,             // NEW VALUE (4√ó increase)
  idleTimeoutMillis: 30000,
  connectionTimeoutMillis: 2000,
  acquireTimeoutMillis: 10000
};
```

**Rationale**:
- Max connections: 50 ‚Üí 200 (eliminates 90% saturation risk)
- Min connections: 10 ‚Üí 20 (reduces cold-start latency)
- Other settings: UNCHANGED (proven stable)

**Configuration File Location**: [RECORD ACTUAL PATH]  
**Git Commit**: [RECORD COMMIT HASH]  
**Timestamp**: [RECORD TIMESTAMP]

---

### Step 3: Apply Change with Zero Downtime

**Method**: Rolling restart (recommended) or hot reload (if supported)

#### Option A: Rolling Restart (Safest)

```bash
# 1. Deploy new configuration to standby instances
ansible-playbook deploy_config.yml --limit standby

# 2. Verify standby instances healthy
curl http://standby-1:3000/health
curl http://standby-2:3000/health

# 3. Switch traffic to standby instances
# (Load balancer drains primary, routes to standby)

# 4. Deploy configuration to primary instances
ansible-playbook deploy_config.yml --limit primary

# 5. Verify primary instances healthy
curl http://primary-1:3000/health
curl http://primary-2:3000/health

# 6. Switch traffic back to primary instances
# (Load balancer routes to primary)

# 7. Verify all instances using new pool size
curl http://primary-1:3000/api/health/pool-config
curl http://standby-1:3000/api/health/pool-config
```

**Expected Downtime**: 0 seconds (rolling restart)

#### Option B: Hot Reload (If Supported)

```bash
# Send SIGHUP to reload configuration without restart
kill -HUP $(cat /var/run/accubooks.pid)

# Verify new pool size active
curl http://localhost:3000/api/health/pool-config
```

**Expected Downtime**: 0 seconds (hot reload)

---

### Step 4: Immediate Verification (0-5 Minutes)

**Action**: Verify change applied successfully

```bash
# 1. Verify pool configuration
curl http://localhost:3000/api/health/pool-config | jq '.pool.max'
# Expected: 200

# 2. Verify active connections
psql -U accubooks -d accubooks_production -c "SELECT count(*) FROM pg_stat_activity WHERE datname='accubooks_production';"
# Expected: 20-40 (min pool size + active queries)

# 3. Verify no errors
tail -100 /var/log/accubooks/app.log | grep -i error
# Expected: No new errors

# 4. Verify latency stable
curl http://localhost:3000/api/health/metrics | jq '.latency.p95'
# Expected: <500ms (no regression)
```

**Pass Criteria**:
- ‚úÖ Pool max = 200
- ‚úÖ No new errors
- ‚úÖ Latency unchanged or improved
- ‚úÖ All instances healthy

**Fail Criteria** (Trigger rollback):
- ‚ùå Pool max ‚â† 200
- ‚ùå Error rate spike >1%
- ‚ùå Latency regression >20%
- ‚ùå Connection thrashing detected

---

### Step 5: Synthetic Load Test (5-15 Minutes)

**Action**: Run 500 synthetic requests to verify performance

```bash
# Run synthetic load test
node scripts/synthetic_load_test.js \
  --requests 500 \
  --concurrency 50 \
  --duration 10m \
  --output post_change_load_test.json

# Analyze results
node scripts/analyze_load_test.js post_change_load_test.json
```

**Expected Results**:
- p50 latency: <200ms
- p95 latency: <500ms
- p99 latency: <1000ms
- Error rate: <0.1%
- Connection pool utilization: <70%

**Pass Criteria**:
- ‚úÖ All metrics within expected range
- ‚úÖ No connection timeouts
- ‚úÖ No pool exhaustion

**Fail Criteria** (Trigger rollback):
- ‚ùå p95 latency >500ms
- ‚ùå Error rate >0.1%
- ‚ùå Connection timeouts detected

---

### Step 6: Sustained Monitoring (15-60 Minutes)

**Action**: Monitor real traffic for 60 minutes

**Metrics to Watch**:
- Active DB connections (should be 20-60 under normal load)
- Pool utilization (should be <70%)
- Connection wait time (should be <10ms p95)
- Error rate (should be <0.1%)
- p95 latency (should be <500ms)

**Alert Thresholds**:
- üü° WARNING: Pool utilization >70%
- üî¥ CRITICAL: Pool utilization >85%
- üî¥ CRITICAL: Error rate >0.5%
- üî¥ CRITICAL: p95 latency >1s

**Evidence Collection**:
- Capture metrics every 5 minutes
- Log all alerts
- Screenshot dashboards at 15, 30, 60 minutes

---

## üîÑ ROLLBACK PLAN

### Trigger Conditions

**IMMEDIATE ROLLBACK** if ANY occur:
- Error rate >1% sustained for 5 minutes
- p95 latency >1s sustained for 5 minutes
- Connection thrashing (rapid connect/disconnect)
- Database CPU >90% sustained
- Any P0 incident triggered

### Rollback Procedure

```bash
# 1. Revert configuration to previous version
git revert [COMMIT_HASH]
git push origin main

# 2. Deploy reverted configuration (rolling restart)
ansible-playbook deploy_config.yml

# 3. Verify rollback successful
curl http://localhost:3000/api/health/pool-config | jq '.pool.max'
# Expected: 50 (original value)

# 4. Monitor for 15 minutes
# Verify metrics return to baseline

# 5. Document rollback reason
echo "Rollback reason: [DESCRIBE ISSUE]" >> launch/evidence/rollback_log.txt
```

**Expected Rollback Time**: 5-10 minutes

---

## ‚úÖ SUCCESS CRITERIA

**Change is considered successful if ALL criteria met**:

1. ‚úÖ Pool max = 200 (verified via API)
2. ‚úÖ Pool utilization <70% under normal load
3. ‚úÖ Connection wait time p95 <10ms
4. ‚úÖ Error rate <0.1% (no regression)
5. ‚úÖ p95 latency <500ms (no regression)
6. ‚úÖ No connection timeouts
7. ‚úÖ No database CPU spike
8. ‚úÖ Sustained for 60 minutes without issues

**If ALL criteria met**: Proceed to Phase 2 (Monitoring Setup)

**If ANY criteria fail**: Execute rollback, investigate, retry

---

## üìä POST-CHANGE VERIFICATION REPORT

**Template** (Complete after implementation):

```markdown
# Database Pool Scaling Verification Report

**Implementation Date**: [TIMESTAMP]
**Operator**: [NAME]
**Change**: Database pool max 50 ‚Üí 200

## Pre-Change Metrics
- Pool utilization: [VALUE]%
- Error rate: [VALUE]%
- p95 latency: [VALUE]ms
- Active connections: [VALUE]

## Post-Change Metrics (60 min sustained)
- Pool utilization: [VALUE]%
- Error rate: [VALUE]%
- p95 latency: [VALUE]ms
- Active connections: [VALUE]

## Synthetic Load Test Results
- Requests: 500
- p50 latency: [VALUE]ms
- p95 latency: [VALUE]ms
- p99 latency: [VALUE]ms
- Error rate: [VALUE]%

## Success Criteria
- [ ] Pool max = 200
- [ ] Pool utilization <70%
- [ ] Error rate <0.1%
- [ ] p95 latency <500ms
- [ ] Sustained 60 min

## Decision
- [X] SUCCESS - Proceed to Phase 2
- [ ] ROLLBACK - Investigate and retry

**Signature**: [NAME]
**Timestamp**: [TIMESTAMP]
```

**Evidence Location**: `launch/evidence/pool_scaling_verification_[TIMESTAMP].md`

---

## üö® INCIDENT RESPONSE

**If issues occur during implementation**:

1. **STOP** - Do not proceed with further changes
2. **ASSESS** - Determine severity (P0/P1/P2)
3. **DECIDE** - Rollback or investigate
4. **EXECUTE** - Follow rollback procedure if needed
5. **DOCUMENT** - Record all details in incident log
6. **NOTIFY** - Alert stakeholders (Executive Operator, Product Lead)

**Incident Log Location**: `launch/evidence/incidents/[TIMESTAMP]_pool_scaling_incident.md`

---

## üìù CHANGE LOG

**Version 1.0.0 ‚Üí 1.0.1**

**Changed**:
- Database connection pool max: 50 ‚Üí 200
- Database connection pool min: 10 ‚Üí 20

**Unchanged**:
- All other pool settings (timeouts, etc.)
- Application code
- Database schema
- API endpoints
- Business logic

**Rationale**: Production Audit identified 90% pool utilization at peak load. Scaling to 200 connections eliminates performance degradation risk while maintaining system integrity.

**Risk Level**: LOW (configuration change only, proven rollback procedure)

---

**End of Database Pool Scaling Implementation Guide**

**Status**: READY FOR EXECUTION  
**Authority**: Production Engineering Lead  
**Next Phase**: Phase 2 - Monitoring Setup (after successful pool scaling)
